%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

Today, HTTP Adaptive Streaming (HAS) is the dominant way of video delivery in the Internet. 
HAS is based on the wide-spread HTTP protocol and takes over its properties such as easy traversal of NAT-devices, firewall-friendliness, encryption in the shape of HTTPS and close-to-customer caching through content-delivery networks (CDNs).
In HAS the video content is split into short chunks (e.g. $2$ seconds) and each chunk is encoded into different quality levels.
The individual chunks are then made available on standard HTTP servers.
The location of the chunks and their encoding are given to the client through a manifest file.
At the beginning of the playback, the streaming client first requests the manifest file. 
Afterward, it chooses the chunks according to its internal adaptation logic, for example based on the current throughput or buffer level.
Dynamic Adaptive Streaming of HTTP (DASH) is an ISO standard which defines the structure and content of such a manifest file and is deployed by major video service providers such as YouTube or Netflix.

As users are shifting away from traditional video broadcast consumption to individual content selection through streaming services, user expectations are growing. 
Users expects the content to be available on all their devices and wherever they go. 
It is well known that stalling events and the video encoding bit-rate, i.e. the video resolution, have a significant impact on the acceptance rate and the Quality of Experience \cite{casas2012youtube}.
Therefore, it is important for the service provider to develop a sophisticated adaptation logic which can prevent stalling events even when faced with bottle-necked or unstable Internet connections, such as cellular access or congested links during after-work hours.

In this paper we take a closer look at the behavior of the adaptation logic of YouTube.
In previous work \cite{sieber16sacrificing} we showed that YouTube's adaptation logic focuses strictly on the user, at the expense of network efficiency.
In particular, we observed that the YouTube player sometimes discards its currently buffered content to re-download it in a higher quality level.
In this way, the player can increase the average quality level shown to the user.
However, the overall efficiency decreases as the previously downloaded segments are discarded.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/eg_request_schedule}%
\caption{Example request schedule from one of the experiment runs \cite{sieber16sacrificing}. From \unit[30]{s} to \unit[90]{s} overlaps can be observed where low quality (144p) is replaced by higher quality levels (240p, 360p and 480p).}
\label{fig:request_schedule}%
\end{figure}

Figure \ref{fig:request_schedule} shows a request schedule for one of the experiment runs.
The x-axis shows the request video interval in playback time, e.g. the first $\SI{16}{\second}$ of the video.
The y-axis shows the time of request based on the experiment time, with $0$ being the time the first HTTP GET request was sent to the server.
At first, the player requests one minute of the lowest quality level.
Then, ${20}$ seconds into the experiment, the player revises its previously made decision, discards two of the low quality segments (i.e. $\SI{30}{\second}$ of playback time) and starts to download a higher quality level instead.
The shaded areas in the figure illustrate where lower quality segments were discarded.
The figure illustrates that in this video view out of \unit[105]{s} of video, approximately \unit[60]{s} were available in more than one quality level at the player. 
As we show in our previous study \cite{sieber16sacrificing}, this is not an isolated incident, but happens on a regular basis. 
Therefore, in this work we take a closer look at the effectiveness of this approach.

The evaluation in this work is based on an experimental data set with over $10.000$ video views of about $30$ different videos.
The videos were played in a testbed where the connection was throttled to $\{0.4, 0.5, \dots \SI{3.0}{Mbps}\}$.
A proxy was used to decrypt the HTTPS connection.
The dataset and testbed is described in detail in \cite{sieber16sacrificing,sieber15costaggressive} and the experimental dataset is freely available online at \cite{traces}. 
Over 70 QoE-relevant metrics such as average quality level, cumulative stalling times and number of quality switches were collected.

Our main contribution lies in analyzing how much the used adaptation algorithm can be optimized. Even if we completely avoid stalling events, a higher mean video quality is achievable in most cases. Further, it is possible to reduce the number of resolution switches and start the video after a shorter initial delay.

The paper is structured as follows. The next section discusses the related work in this area of research. 
In Section \ref{sec:sysmodel} we discuss the methodology and the two approaches used in this work.
In Section \ref{sec:results} we present the results and in Section \ref{sec:conclusion} we conclude this work and outline future work.

